{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1d87de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_photos = pd.DataFrame(pd.read_csv(\"pattern_info.csv\")).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22246082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'name', 'permalink', 'square_url', 'small_url'], dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_photos.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286b3382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "images = data_photos[\"photos_square_url\"]\n",
    "image_url = images.iloc[0]\n",
    "response = requests.get(image_url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "convert_tensor = transforms.ToTensor()\n",
    "img_tensor = convert_tensor(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc782227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65, 5121, 5442, 2246, 2768, 5513, 4672, 703, 5272, 3781, 439, 1621, 1481, 6222, 1250, 4102, 1040, 1700, 6045, 3407, 3829, 6522, 1095, 2847, 3444, 4927, 129, 5231, 4155, 535, 300, 3241, 5153, 5217, 2890, 6493, 5638, 4348, 1211, 1593, 756, 919, 1519, 3142, 568, 6264, 479, 5378, 3398, 1768, 2655, 2548, 3663, 4839, 3943, 4593, 5694, 5080, 2565, 5695, 3595, 6373, 1060, 462, 3168, 4139, 1979, 1467, 5415, 5093, 3110, 4194, 5564, 824, 655, 877, 2134, 6251, 2909, 2813, 1753, 5056, 626, 4911, 1043, 3331, 3236, 4242, 2696, 4807, 6573, 4200, 433, 1558, 6099, 5182, 29, 4333, 2619, 5631]\n",
      "check\n",
      "Zoro,'Nothing happened' x 62\n",
      "(4938, 3)\n",
      "torch.Size([4938, 3, 75, 75])\n"
     ]
    }
   ],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import random\n",
    "\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "## splitting the data for ease\n",
    "labels = data_photos[['name', 'permalink']]\n",
    "labels[\"type_encoded\"] =  encoder.fit_transform(labels['permalink'])\n",
    "images = data_photos[\"small_url\"]\n",
    "\n",
    "## Running counters\n",
    "fail_counter = 0\n",
    "success_counter = 0\n",
    "clean_index = []\n",
    "\n",
    "## Hyperparameters?\n",
    "n_values = 5000\n",
    "possible_values = range(0,data_photos.shape[0])\n",
    "random_set = random.sample(possible_values,n_values)\n",
    "print(random_set[0:100])\n",
    "\n",
    "for i in random_set:\n",
    "    ## Try needed as some links are broken or go to no longer existing photos\n",
    "    try:\n",
    "        ## Loading in Image through URL\n",
    "        image_url = images.iloc[i]\n",
    "        response = requests.get(image_url)\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        ## Tensoring it\n",
    "        convert_tensor = transforms.ToTensor()\n",
    "        img_tensor = convert_tensor(img)\n",
    "        ## Resizing and joining it to others\n",
    "        # they all need to be the same size so I'm just sayin 250x250 lmao\n",
    "        #resize=torchvision.transforms.Resize((250,250))\n",
    "        # img_resize = resize(img_tensor)\n",
    "        ## First round of making the clean images a tensor\n",
    "        if success_counter == 0:\n",
    "            clean_images = img_tensor\n",
    "            success_counter = success_counter + 1\n",
    "        ## second round of stacking the tensors to add another Batch dimension\n",
    "        elif success_counter == 1:\n",
    "            clean_images = torch.stack([clean_images,img_tensor])\n",
    "            success_counter = success_counter + 1\n",
    "            print(\"check\")\n",
    "        ## For the rest of the rounds concatinating the code\n",
    "        else:\n",
    "            img_tensor = img_tensor.unsqueeze(dim = 3).permute(3,0,1, 2)\n",
    "            clean_images = torch.concat([clean_images,img_tensor])\n",
    "        ## Making sure Labels are up to date\n",
    "        clean_index.append(i)\n",
    "    except:\n",
    "        ## To see how many we lost\n",
    "        fail_counter += 1\n",
    "\n",
    "clean_labels = labels.iloc[clean_index,:]\n",
    "print(\"Zoro,'Nothing happened' x \" + str(fail_counter))\n",
    "print(clean_labels.shape)\n",
    "print(clean_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "eb1cb2c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65           tee\n",
       "5121       shawl\n",
       "5442    cardigan\n",
       "2246       child\n",
       "2768    camisole\n",
       "          ...   \n",
       "621     pullover\n",
       "4570    pullover\n",
       "3969    pullover\n",
       "3298         tee\n",
       "5206    camisole\n",
       "Name: permalink, Length: 4938, dtype: object"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_labels.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "72588143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4938, 20)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_label = pd.get_dummies(clean_labels, columns = [\"permalink\"], prefix = \"type\", dtype = float)\n",
    "clean_label = clean_label.iloc[:,2:]\n",
    "clean_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1371dc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3949, 3, 75, 75])\n",
      "torch.Size([988, 3, 75, 75])\n",
      "torch.Size([3949, 20])\n",
      "torch.Size([988, 20])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "num_final = clean_labels.shape[0]\n",
    "split_val = .8\n",
    "\n",
    "\n",
    "train_images = clean_images[1:round(split_val*num_final),...].float()\n",
    "test_images = clean_images[round(split_val*num_final):num_final,...].float()\n",
    "\n",
    "train_labels = torch.from_numpy(clean_label.iloc[1:round(split_val*num_final),:].values).float()\n",
    "test_labels = torch.from_numpy(clean_label.iloc[round(split_val*num_final):num_final,:].values).float()\n",
    "\n",
    "mean = np.mean(clean_labels.iloc[:,2].values)\n",
    "std = np.std(clean_labels.iloc[:,2].values)\n",
    "#train_labels = (train_labels - mean) / std\n",
    "#test_labels = (test_labels - mean) / std\n",
    "\n",
    "\n",
    "print(train_images.shape)\n",
    "print(test_images.shape)\n",
    "\n",
    "print(train_labels.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "67256616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14.,  4.,  5.,  ...,  3., 17.,  2.])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "345b9fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "95d4b920",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "batch_size_mod = 10\n",
    "batch_size = round(train_labels.shape[0]/batch_size_mod)\n",
    "\n",
    "loader = data.DataLoader(data.TensorDataset(train_images, train_labels), batch_size=batch_size)\n",
    "testloader = data.DataLoader(data.TensorDataset(test_images, test_labels), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7cb27f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class ConvNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNeuralNet, self).__init__()\n",
    "        ## Xavier weight initialization\n",
    "        self.layer1 = nn.Sequential(\n",
    "            # adjusting input size for the 3 layered image\n",
    "            nn.Conv2d(3, 6, 5),\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
    "        ## Xavier initial weights\n",
    "        nn.init.xavier_normal_(self.layer1[0].weight)\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, 7),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.fc = nn.Linear(3136, 1000)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(.01)\n",
    "        self.fc1 = nn.Linear(1000, 100)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(100, 20)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.relu(out)\n",
    "        ## Droupout layer\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "14e10b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNeuralNet()\n",
    "\n",
    "## Established loss function\n",
    "cost = nn.CrossEntropyLoss()\n",
    "## SGD as optimizer, L@ regularization brought accuracy down\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=.0001,momentum=0.9)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "60b2b146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Step [1/10], Loss: 2.8403\n",
      "Epoch [1/15], Step [2/10], Loss: 2.8429\n",
      "Epoch [1/15], Step [3/10], Loss: 2.8351\n",
      "Epoch [1/15], Step [4/10], Loss: 2.8376\n",
      "Epoch [1/15], Step [5/10], Loss: 2.8293\n",
      "Epoch [1/15], Step [6/10], Loss: 2.8310\n",
      "Epoch [1/15], Step [7/10], Loss: 2.8431\n",
      "Epoch [1/15], Step [8/10], Loss: 2.8489\n",
      "Epoch [1/15], Step [9/10], Loss: 2.8478\n",
      "Epoch [1/15], Step [10/10], Loss: 2.8269\n",
      "Epoch [2/15], Step [1/10], Loss: 2.8408\n",
      "Epoch [2/15], Step [2/10], Loss: 2.8427\n",
      "Epoch [2/15], Step [3/10], Loss: 2.8352\n",
      "Epoch [2/15], Step [4/10], Loss: 2.8378\n",
      "Epoch [2/15], Step [5/10], Loss: 2.8296\n",
      "Epoch [2/15], Step [6/10], Loss: 2.8322\n",
      "Epoch [2/15], Step [7/10], Loss: 2.8433\n",
      "Epoch [2/15], Step [8/10], Loss: 2.8486\n",
      "Epoch [2/15], Step [9/10], Loss: 2.8475\n",
      "Epoch [2/15], Step [10/10], Loss: 2.8279\n",
      "Epoch [3/15], Step [1/10], Loss: 2.8402\n",
      "Epoch [3/15], Step [2/10], Loss: 2.8426\n",
      "Epoch [3/15], Step [3/10], Loss: 2.8353\n",
      "Epoch [3/15], Step [4/10], Loss: 2.8372\n",
      "Epoch [3/15], Step [5/10], Loss: 2.8298\n",
      "Epoch [3/15], Step [6/10], Loss: 2.8307\n",
      "Epoch [3/15], Step [7/10], Loss: 2.8431\n",
      "Epoch [3/15], Step [8/10], Loss: 2.8487\n",
      "Epoch [3/15], Step [9/10], Loss: 2.8469\n",
      "Epoch [3/15], Step [10/10], Loss: 2.8281\n",
      "Epoch [4/15], Step [1/10], Loss: 2.8403\n",
      "Epoch [4/15], Step [2/10], Loss: 2.8424\n",
      "Epoch [4/15], Step [3/10], Loss: 2.8350\n",
      "Epoch [4/15], Step [4/10], Loss: 2.8370\n",
      "Epoch [4/15], Step [5/10], Loss: 2.8287\n",
      "Epoch [4/15], Step [6/10], Loss: 2.8304\n",
      "Epoch [4/15], Step [7/10], Loss: 2.8419\n",
      "Epoch [4/15], Step [8/10], Loss: 2.8486\n",
      "Epoch [4/15], Step [9/10], Loss: 2.8473\n",
      "Epoch [4/15], Step [10/10], Loss: 2.8282\n",
      "Epoch [5/15], Step [1/10], Loss: 2.8399\n",
      "Epoch [5/15], Step [2/10], Loss: 2.8427\n",
      "Epoch [5/15], Step [3/10], Loss: 2.8354\n",
      "Epoch [5/15], Step [4/10], Loss: 2.8379\n",
      "Epoch [5/15], Step [5/10], Loss: 2.8298\n",
      "Epoch [5/15], Step [6/10], Loss: 2.8314\n",
      "Epoch [5/15], Step [7/10], Loss: 2.8425\n",
      "Epoch [5/15], Step [8/10], Loss: 2.8493\n",
      "Epoch [5/15], Step [9/10], Loss: 2.8471\n",
      "Epoch [5/15], Step [10/10], Loss: 2.8279\n",
      "Epoch [6/15], Step [1/10], Loss: 2.8408\n",
      "Epoch [6/15], Step [2/10], Loss: 2.8426\n",
      "Epoch [6/15], Step [3/10], Loss: 2.8351\n",
      "Epoch [6/15], Step [4/10], Loss: 2.8367\n",
      "Epoch [6/15], Step [5/10], Loss: 2.8299\n",
      "Epoch [6/15], Step [6/10], Loss: 2.8317\n",
      "Epoch [6/15], Step [7/10], Loss: 2.8433\n",
      "Epoch [6/15], Step [8/10], Loss: 2.8485\n",
      "Epoch [6/15], Step [9/10], Loss: 2.8478\n",
      "Epoch [6/15], Step [10/10], Loss: 2.8272\n",
      "Epoch [7/15], Step [1/10], Loss: 2.8407\n",
      "Epoch [7/15], Step [2/10], Loss: 2.8423\n",
      "Epoch [7/15], Step [3/10], Loss: 2.8350\n",
      "Epoch [7/15], Step [4/10], Loss: 2.8372\n",
      "Epoch [7/15], Step [5/10], Loss: 2.8300\n",
      "Epoch [7/15], Step [6/10], Loss: 2.8310\n",
      "Epoch [7/15], Step [7/10], Loss: 2.8436\n",
      "Epoch [7/15], Step [8/10], Loss: 2.8490\n",
      "Epoch [7/15], Step [9/10], Loss: 2.8477\n",
      "Epoch [7/15], Step [10/10], Loss: 2.8273\n",
      "Epoch [8/15], Step [1/10], Loss: 2.8401\n",
      "Epoch [8/15], Step [2/10], Loss: 2.8423\n",
      "Epoch [8/15], Step [3/10], Loss: 2.8364\n",
      "Epoch [8/15], Step [4/10], Loss: 2.8375\n",
      "Epoch [8/15], Step [5/10], Loss: 2.8294\n",
      "Epoch [8/15], Step [6/10], Loss: 2.8308\n",
      "Epoch [8/15], Step [7/10], Loss: 2.8431\n",
      "Epoch [8/15], Step [8/10], Loss: 2.8476\n",
      "Epoch [8/15], Step [9/10], Loss: 2.8476\n",
      "Epoch [8/15], Step [10/10], Loss: 2.8272\n",
      "Epoch [9/15], Step [1/10], Loss: 2.8404\n",
      "Epoch [9/15], Step [2/10], Loss: 2.8425\n",
      "Epoch [9/15], Step [3/10], Loss: 2.8347\n",
      "Epoch [9/15], Step [4/10], Loss: 2.8378\n",
      "Epoch [9/15], Step [5/10], Loss: 2.8304\n",
      "Epoch [9/15], Step [6/10], Loss: 2.8313\n",
      "Epoch [9/15], Step [7/10], Loss: 2.8435\n",
      "Epoch [9/15], Step [8/10], Loss: 2.8493\n",
      "Epoch [9/15], Step [9/10], Loss: 2.8474\n",
      "Epoch [9/15], Step [10/10], Loss: 2.8280\n",
      "Epoch [10/15], Step [1/10], Loss: 2.8408\n",
      "Epoch [10/15], Step [2/10], Loss: 2.8430\n",
      "Epoch [10/15], Step [3/10], Loss: 2.8351\n",
      "Epoch [10/15], Step [4/10], Loss: 2.8362\n",
      "Epoch [10/15], Step [5/10], Loss: 2.8298\n",
      "Epoch [10/15], Step [6/10], Loss: 2.8320\n",
      "Epoch [10/15], Step [7/10], Loss: 2.8418\n",
      "Epoch [10/15], Step [8/10], Loss: 2.8498\n",
      "Epoch [10/15], Step [9/10], Loss: 2.8469\n",
      "Epoch [10/15], Step [10/10], Loss: 2.8287\n",
      "Epoch [11/15], Step [1/10], Loss: 2.8416\n",
      "Epoch [11/15], Step [2/10], Loss: 2.8421\n",
      "Epoch [11/15], Step [3/10], Loss: 2.8349\n",
      "Epoch [11/15], Step [4/10], Loss: 2.8364\n",
      "Epoch [11/15], Step [5/10], Loss: 2.8301\n",
      "Epoch [11/15], Step [6/10], Loss: 2.8315\n",
      "Epoch [11/15], Step [7/10], Loss: 2.8433\n",
      "Epoch [11/15], Step [8/10], Loss: 2.8494\n",
      "Epoch [11/15], Step [9/10], Loss: 2.8471\n",
      "Epoch [11/15], Step [10/10], Loss: 2.8264\n",
      "Epoch [12/15], Step [1/10], Loss: 2.8411\n",
      "Epoch [12/15], Step [2/10], Loss: 2.8427\n",
      "Epoch [12/15], Step [3/10], Loss: 2.8352\n",
      "Epoch [12/15], Step [4/10], Loss: 2.8369\n",
      "Epoch [12/15], Step [5/10], Loss: 2.8291\n",
      "Epoch [12/15], Step [6/10], Loss: 2.8315\n",
      "Epoch [12/15], Step [7/10], Loss: 2.8424\n",
      "Epoch [12/15], Step [8/10], Loss: 2.8492\n",
      "Epoch [12/15], Step [9/10], Loss: 2.8469\n",
      "Epoch [12/15], Step [10/10], Loss: 2.8280\n",
      "Epoch [13/15], Step [1/10], Loss: 2.8409\n",
      "Epoch [13/15], Step [2/10], Loss: 2.8426\n",
      "Epoch [13/15], Step [3/10], Loss: 2.8350\n",
      "Epoch [13/15], Step [4/10], Loss: 2.8370\n",
      "Epoch [13/15], Step [5/10], Loss: 2.8301\n",
      "Epoch [13/15], Step [6/10], Loss: 2.8311\n",
      "Epoch [13/15], Step [7/10], Loss: 2.8428\n",
      "Epoch [13/15], Step [8/10], Loss: 2.8487\n",
      "Epoch [13/15], Step [9/10], Loss: 2.8474\n",
      "Epoch [13/15], Step [10/10], Loss: 2.8276\n",
      "Epoch [14/15], Step [1/10], Loss: 2.8401\n",
      "Epoch [14/15], Step [2/10], Loss: 2.8428\n",
      "Epoch [14/15], Step [3/10], Loss: 2.8358\n",
      "Epoch [14/15], Step [4/10], Loss: 2.8379\n",
      "Epoch [14/15], Step [5/10], Loss: 2.8291\n",
      "Epoch [14/15], Step [6/10], Loss: 2.8309\n",
      "Epoch [14/15], Step [7/10], Loss: 2.8433\n",
      "Epoch [14/15], Step [8/10], Loss: 2.8487\n",
      "Epoch [14/15], Step [9/10], Loss: 2.8477\n",
      "Epoch [14/15], Step [10/10], Loss: 2.8277\n",
      "Epoch [15/15], Step [1/10], Loss: 2.8405\n",
      "Epoch [15/15], Step [2/10], Loss: 2.8431\n",
      "Epoch [15/15], Step [3/10], Loss: 2.8350\n",
      "Epoch [15/15], Step [4/10], Loss: 2.8370\n",
      "Epoch [15/15], Step [5/10], Loss: 2.8298\n",
      "Epoch [15/15], Step [6/10], Loss: 2.8315\n",
      "Epoch [15/15], Step [7/10], Loss: 2.8436\n",
      "Epoch [15/15], Step [8/10], Loss: 2.8495\n",
      "Epoch [15/15], Step [9/10], Loss: 2.8471\n",
      "Epoch [15/15], Step [10/10], Loss: 2.8283\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "\n",
    "min_loss = 1000000\n",
    "\n",
    "total_step = len(loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(loader):  \n",
    "        \n",
    "\n",
    "        images, labels = data\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = cost(outputs, labels)\n",
    "            \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                        .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "        if loss.item() < min_loss:\n",
    "            torch.save(model.state_dict(), 'ideal_model.pth')\n",
    "            min_loss = loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "588b7c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.8424, grad_fn=<DivBackward1>)\n",
      "tensor(0.2861)\n"
     ]
    }
   ],
   "source": [
    "model = ConvNeuralNet()\n",
    "model.load_state_dict(torch.load(\"ideal_model.pth\"))\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, y_train = next(dataiter)\n",
    "y_pred = model(images)\n",
    "\n",
    "ce = cost(y_pred, y_train)\n",
    "acc = (torch.argmax(y_pred, 1) == torch.argmax(y_train, 1)).float().mean()\n",
    "\n",
    "print(ce)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "63efc87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10 test images: 28.607595443725586 %\n",
      "Accuracy of the network on the 10 test images: 29.620254516601562 %\n",
      "Accuracy of the network on the 10 test images: 30.30303192138672 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = model(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        acc = (torch.argmax(outputs, 1) == torch.argmax(labels, 1)).float().mean()\n",
    "\n",
    "        print(f'Accuracy of the network on the 10 test images: {acc*100} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsan6600",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
