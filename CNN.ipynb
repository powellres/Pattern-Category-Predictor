{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1d87de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_photos = pd.DataFrame(pd.read_csv(\"pattern_info.csv\")).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22246082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'name', 'permalink', 'square_url', 'small_url'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_photos.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "286b3382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "#images = data_photos[\"photos_square_url\"]\n",
    "#image_url = images.iloc[0]\n",
    "##response = requests.get(image_url)\n",
    "#img = Image.open(BytesIO(response.content))\n",
    "#convert_tensor = transforms.ToTensor()\n",
    "#img_tensor = convert_tensor(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc782227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[594, 1938, 5193, 1194, 4352, 1918, 42, 593, 3043, 3085, 65, 1306, 5646, 1454, 5000, 1632, 1474, 4540, 3622, 2285, 5206, 3006, 2836, 2329, 3409, 6047, 736, 741, 3757, 2432, 1751, 4610, 580, 4747, 6096, 4857, 6031, 2968, 2095, 4894, 5024, 2466, 2746, 3367, 1072, 3424, 4133, 2886, 1092, 5349, 4963, 2572, 615, 3246, 3917, 1561, 5245, 4608, 2139, 1954, 6468, 5822, 3984, 4245, 1557, 6217, 1464, 312, 5386, 5412, 461, 5047, 4401, 1941, 5814, 3345, 5379, 355, 2826, 1313, 148, 3116, 5005, 872, 2266, 2470, 3530, 288, 219, 1298, 3670, 4193, 5447, 4218, 2193, 5483, 3279, 310, 32, 6244]\n",
      "check\n",
      "Zoro,'Nothing happened' x 3951\n",
      "(1049, 3)\n",
      "torch.Size([1049, 3, 240, 180])\n"
     ]
    }
   ],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import random\n",
    "\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "## splitting the data for ease\n",
    "labels = data_photos[['name', 'permalink']]\n",
    "labels[\"type_encoded\"] =  encoder.fit_transform(labels['permalink'])\n",
    "images = data_photos[\"small_url\"]\n",
    "\n",
    "## Running counters\n",
    "fail_counter = 0\n",
    "success_counter = 0\n",
    "clean_index = []\n",
    "\n",
    "## Hyperparameters?\n",
    "n_values = 5000\n",
    "possible_values = range(0,data_photos.shape[0])\n",
    "random_set = random.sample(possible_values,n_values)\n",
    "print(random_set[0:100])\n",
    "\n",
    "for i in random_set:\n",
    "    ## Try needed as some links are broken or go to no longer existing photos\n",
    "    try:\n",
    "        ## Loading in Image through URL\n",
    "        image_url = images.iloc[i]\n",
    "        response = requests.get(image_url)\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        ## Tensoring it\n",
    "        convert_tensor = transforms.ToTensor()\n",
    "        img_tensor = convert_tensor(img)\n",
    "        ## Resizing and joining it to others\n",
    "        # they all need to be the same size so I'm just sayin 250x250 lmao\n",
    "        #resize=torchvision.transforms.Resize((250,250))\n",
    "        # img_resize = resize(img_tensor)\n",
    "        ## First round of making the clean images a tensor\n",
    "        if success_counter == 0:\n",
    "            clean_images = img_tensor\n",
    "            success_counter = success_counter + 1\n",
    "        ## second round of stacking the tensors to add another Batch dimension\n",
    "        elif success_counter == 1:\n",
    "            clean_images = torch.stack([clean_images,img_tensor])\n",
    "            success_counter = success_counter + 1\n",
    "            print(\"check\")\n",
    "        ## For the rest of the rounds concatinating the code\n",
    "        else:\n",
    "            img_tensor = img_tensor.unsqueeze(dim = 3).permute(3,0,1, 2)\n",
    "            clean_images = torch.concat([clean_images,img_tensor])\n",
    "        ## Making sure Labels are up to date\n",
    "        clean_index.append(i)\n",
    "    except:\n",
    "        ## To see how many we lost\n",
    "        fail_counter += 1\n",
    "\n",
    "clean_labels = labels.iloc[clean_index,:]\n",
    "print(\"Zoro,'Nothing happened' x \" + str(fail_counter))\n",
    "print(clean_labels.shape)\n",
    "print(clean_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb1cb2c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "594     cardigan\n",
       "4352    camisole\n",
       "1454         tee\n",
       "736     pullover\n",
       "741     pullover\n",
       "          ...   \n",
       "747     pullover\n",
       "6077    pullover\n",
       "5451         tee\n",
       "5699       scarf\n",
       "3715    camisole\n",
       "Name: permalink, Length: 1049, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_labels.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72588143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1049, 18)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_label = pd.get_dummies(clean_labels, columns = [\"permalink\"], prefix = \"type\", dtype = float)\n",
    "clean_label = clean_label.iloc[:,2:]\n",
    "clean_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1371dc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([838, 3, 240, 180])\n",
      "torch.Size([210, 3, 240, 180])\n",
      "torch.Size([838, 18])\n",
      "torch.Size([210, 18])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "num_final = clean_labels.shape[0]\n",
    "split_val = .8\n",
    "\n",
    "\n",
    "train_images = clean_images[1:round(split_val*num_final),...].float()\n",
    "test_images = clean_images[round(split_val*num_final):num_final,...].float()\n",
    "\n",
    "train_labels = torch.from_numpy(clean_label.iloc[1:round(split_val*num_final),:].values).float()\n",
    "test_labels = torch.from_numpy(clean_label.iloc[round(split_val*num_final):num_final,:].values).float()\n",
    "\n",
    "mean = np.mean(clean_labels.iloc[:,2].values)\n",
    "std = np.std(clean_labels.iloc[:,2].values)\n",
    "#train_labels = (train_labels - mean) / std\n",
    "#test_labels = (test_labels - mean) / std\n",
    "\n",
    "\n",
    "print(train_images.shape)\n",
    "print(test_images.shape)\n",
    "\n",
    "print(train_labels.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67256616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "345b9fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95d4b920",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "batch_size_mod = 10\n",
    "batch_size = round(train_labels.shape[0]/batch_size_mod)\n",
    "\n",
    "loader = data.DataLoader(data.TensorDataset(train_images, train_labels), batch_size=batch_size)\n",
    "testloader = data.DataLoader(data.TensorDataset(test_images, test_labels), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7cb27f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class ConvNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNeuralNet, self).__init__()\n",
    "        ## Xavier weight initialization\n",
    "        self.layer1 = nn.Sequential(\n",
    "            # adjusting input size for the 3 layered image\n",
    "            nn.Conv2d(3, 6, 5),\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
    "        ## Xavier initial weights\n",
    "        nn.init.xavier_normal_(self.layer1[0].weight)\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, 7),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.fc = nn.Linear(35200, 10000)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(.01)\n",
    "        self.fc1 = nn.Linear(10000, 1000)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(1000, 20)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.relu(out)\n",
    "        ## Droupout layer\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14e10b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNeuralNet()\n",
    "\n",
    "## Established loss function\n",
    "cost = nn.CrossEntropyLoss()\n",
    "## SGD as optimizer, L@ regularization brought accuracy down\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=.0001,momentum=0.9)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60b2b146",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "0D or 1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#Forward pass\u001b[39;00m\n\u001b[0;32m     14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m---> 15\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcost\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n\u001b[0;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\duckd\\anaconda3\\envs\\dsan6600\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\duckd\\anaconda3\\envs\\dsan6600\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\duckd\\anaconda3\\envs\\dsan6600\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\duckd\\anaconda3\\envs\\dsan6600\\lib\\site-packages\\torch\\nn\\functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3052\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "\n",
    "min_loss = 1000000\n",
    "\n",
    "total_step = len(loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(loader):  \n",
    "        \n",
    "\n",
    "        images, labels = data\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = cost(outputs, labels)\n",
    "            \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                        .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "        if loss.item() < min_loss:\n",
    "            torch.save(model.state_dict(), 'ideal_model.pth')\n",
    "            min_loss = loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588b7c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.8424, grad_fn=<DivBackward1>)\n",
      "tensor(0.2861)\n"
     ]
    }
   ],
   "source": [
    "model = ConvNeuralNet()\n",
    "model.load_state_dict(torch.load(\"ideal_model.pth\"))\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, y_train = next(dataiter)\n",
    "y_pred = model(images)\n",
    "\n",
    "ce = cost(y_pred, y_train)\n",
    "acc = (torch.argmax(y_pred, 1) == torch.argmax(y_train, 1)).float().mean()\n",
    "\n",
    "print(ce)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63efc87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10 test images: 28.607595443725586 %\n",
      "Accuracy of the network on the 10 test images: 29.620254516601562 %\n",
      "Accuracy of the network on the 10 test images: 30.30303192138672 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = model(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        acc = (torch.argmax(outputs, 1) == torch.argmax(labels, 1)).float().mean()\n",
    "\n",
    "        print(f'Accuracy of the network on the 10 test images: {acc*100} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsan6600",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
